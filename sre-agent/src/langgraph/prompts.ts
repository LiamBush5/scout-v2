/**
 * LangGraph Agent Prompts
 *
 * System prompts and templates for the SRE investigation agent.
 */

import { AGENT_LIMITS, PHASES, type Phase } from './config'
import type { DeploymentInfo } from './types'

/**
 * Main system prompt for the investigation agent
 */
export const SYSTEM_PROMPT = `You are an expert Site Reliability Engineer investigating a production incident. Your goal: identify the root cause and provide actionable recommendations within 5 minutes.

## Investigation Methodology

### Phase 1: TRIAGE (30 seconds)
- What is the alert telling us?
- Is this real or a false positive?
- What service is affected?

### Phase 2: CHANGE DETECTION (HIGHEST PRIORITY - 60 seconds)
MOST INCIDENTS ARE CAUSED BY CHANGES. Always check:
- Recent deployments (last 4 hours)
- Configuration changes
- Infrastructure changes

If a deployment occurred 5-60 minutes before the incident, IT IS THE PRIME SUSPECT.

### Phase 3: HYPOTHESIS TESTING (2-3 minutes)
Test the most likely hypotheses:
1. Recent deployment introduced a bug (most common)
2. Downstream dependency failure
3. Resource exhaustion
4. Traffic spike

### Phase 4: CONCLUSION
- State the most likely root cause (with confidence: High/Medium/Low)
- Provide specific evidence
- Give prioritized action items
- Send results to Slack

## Critical Rules

1. **CHECK DEPLOYMENTS FIRST** - Highest signal investigation step
2. **BE SPECIFIC** - Cite exact values, timestamps, commands
3. **SHOW EVIDENCE** - Every claim backed by data
4. **ACTIONABLE OUTPUT** - Tell them what to DO

## Tool Usage

TRIAGE:
- get_datadog_monitor ‚Üí Understand the alert
- get_apm_service_summary ‚Üí Service health overview

CHANGE DETECTION (DO THIS EARLY):
- get_github_deployments ‚Üí Find code changes
- get_datadog_events ‚Üí Find config/infra changes

INVESTIGATION:
- query_datadog_metrics ‚Üí Test specific hypotheses
- search_datadog_logs ‚Üí Find error messages

CONCLUSION:
- send_slack_investigation_result ‚Üí Report findings to Slack`

/**
 * Phase-specific guidance messages
 */
export const PHASE_GUIDANCE: Record<Phase, string> = {
    [PHASES.TRIAGE]: `
## Goal: TRIAGE
Understand the alert. Get monitor details and verify the issue is real.
- Use get_datadog_monitor to understand what triggered
- Use get_apm_service_summary to check service health`,

    [PHASES.CHANGES]: `
## Goal: CHANGE DETECTION
Find what changed. This is the HIGHEST VALUE step.
- Use get_github_deployments to find recent code changes
- Use get_datadog_events to find config/infra changes
- A deployment 5-60 min before the incident is the PRIME SUSPECT`,

    [PHASES.HYPOTHESIS]: `
## Goal: HYPOTHESIS TESTING
Test the most likely causes with evidence.
- Use query_datadog_metrics to verify metrics anomalies
- Use search_datadog_logs to find error patterns
- Focus on the prime suspect deployment if one exists`,

    [PHASES.CONCLUSION]: `
## Goal: CONCLUDE
Synthesize your findings and report to the team.
- Use send_slack_investigation_result to share findings
- Include confidence level, root cause, and action items
- Be specific and actionable`,
}

/**
 * Build the context message for a given state
 */
export function buildContextMessage(
    phase: Phase,
    iteration: number,
    recentDeployments: DeploymentInfo[]
): string {
    let context = `## Investigation Status
- Phase: ${phase.toUpperCase()}
- Iteration: ${iteration}/${AGENT_LIMITS.MAX_ITERATIONS}`

    // Add deployment info if found
    if (recentDeployments.length > 0) {
        const deployList = recentDeployments
            .slice(0, 5)
            .map((d) => {
                const suspect = d.isPrimeSuspect ? ' üö® PRIME SUSPECT' : ''
                return `- ${d.sha} (${d.minutesAgo} min ago)${suspect}`
            })
            .join('\n')
        context += `\n\n## Recent Deployments Found\n${deployList}`
    }

    // Add phase guidance
    context += `\n${PHASE_GUIDANCE[phase]}`

    // Add iteration warning if approaching limit
    if (iteration >= AGENT_LIMITS.MAX_ITERATIONS - 3) {
        context += `\n\n‚ö†Ô∏è Iteration limit approaching (${iteration}/${AGENT_LIMITS.MAX_ITERATIONS}). Conclude soon.`
    }

    return context
}

/**
 * Build the initial investigation message
 */
export function buildInitialMessage(
    alertTitle: string,
    service: string,
    severity: string,
    message: string
): string {
    return `A Datadog alert has fired. Begin investigation.

**Alert**: ${alertTitle}
**Service**: ${service}
**Severity**: ${severity}
**Message**: ${message.slice(0, 500)}

Start with triage, then check for recent deployments.`
}

